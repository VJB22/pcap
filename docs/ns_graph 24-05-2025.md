### **MAIN IDEA:**

## **Complete Pipeline for Artifact Inference**

graph: **Workload ↔ Device ↔ Workload**  
goal: **Rank deployment artifacts per full chain system** using:

**layered pipeline (v1)**

Community Detection (Louvain) (done)

      ↓

Community Profiles (avg metrics per community) (done)

      ↓

Graph Metrics (degree, betweenness, closeness) (done)

      ↓

Role Mining (backend, frontend, orchestrator) (done)

      ↓

GNN (learns patterns from features \+ graph) done 

      ↓

Final Artifact Ranking (combines everything) done

### **Overall Goal: PCAP Artifact Inference Pipeline**

**Purpose**:  
 To analyze network communication data (PCAP) and infer **likely deployment artifacts** for workloads \+ devices—whether they are **baremetal, VMs, containers, serverless**, or specialized types like **orchestrated containers** and **mini VMs**.  
This helps **optimize cloud migration**, understand **system architecture**, and guide deployment strategies.

### **1 Graph Construction**

**Why**: Model communication patterns as a **graph** where:

* **Workloads** are nodes.  
* **Device roles** (like switches, routers) act as **hubs** connecting workloads, intermediate nodes.  
* Edges represent communication, enriched with metrics like bytes, session length, etc.(rows)

**What**:

* Load PCAP data (`df` from Parquet).  
* For each row:

  * Add source and destination **workload nodes**.  
  * Add a **device node** (`dst_role`).  
  * Add **edges** between workloads and devices, storing network features as attributes.  
    

### **2 Community Detection with Louvain**

**Why**:  
To discover **groups of nodes** (workloads \+ devices) that communicate densely, hinting at **logical deployment units**.

**What**:

* Use `community_louvain.best_partition()` to assign a **community ID** to each node.  
* Store these IDs as node attributes.  
* Group nodes into `community_dict` for further analysis.

### **3 Community Profiles**

**Why**:  
 To summarize **behavioral patterns** within each community—average session lengths, burstiness, bytes sent, etc.—useful for later **role mining** and **artifact inference**.

**What**:

* For each community:

  * Aggregate edge metrics like session length, bytes, TTL variability.  
  * Compute averages (handle NaNs safely).  
  * Assign these profiles back to **all nodes** in the community.

**How**:

* Store per-community profiles in `community_profiles`.

### **4 Graph Metrics Computation**

**Why**:

Graph metrics like **degree, betweenness, closeness, pagerank** capture **structural roles** in the network (e.g., central servers vs. endpoints).

**What**:

* Compute and assign these metrics as node attributes.

**How**:

* Use NetworkX’s built-in functions (e.g., `degree()`, `betweenness_centrality()`) and store results per node.

### **5 Role Mining**

**Why**:  
 To label each node with a **functional role** (e.g., backend, frontend, orchestrator) based on:

* **Graph metrics**.  
* **Community profiles**.  
* **Device types** (for device nodes).

**What**:

* Rules like:

  * High degree \+ betweenness \= orchestrator.  
  * High burstiness \= frontend.  
    High session length \+ low entropy \= backend.

**How**:

* The `assign_role()` function encapsulates logic.  
* Applied to all nodes via iteration.

### **6 GNN Preparation (Per-Community Batches)**

**Why**:  
 To **learn latent patterns** in communication behavior using **Graph Neural Networks** (GNNs), leveraging node features \+ graph structure.

**What**:

* For each community:

  * Build a **PyTorch Geometric `Data` object**.  
  * Include features: graph metrics, community info, system flags (e.g., `is_physical_machine`), and role one-hot encoding.  
  * Labels: weak supervision from `artifact_type_top` if available.

**How**:

* `gnn_batches` stores all per-community `Data` objects.

### **7 GNN Model & Training**

**Why**:  
Learn from **graph-structured data** to predict **artifact types** based on structural and behavioral features.

**What**:

* A **GraphSAGE** model (`ArtifactGNN`) with two convolution layers \+ a linear layer.  
* Cross-entropy loss on available labels (`-1` indicates unknown).

**How**:

* Train on **community batches** for **memory efficiency**.  
* Every 20 epochs: print average loss across valid batches.

### **8 Mapping Predictions Back**

**Why**:  
 Store GNN predictions (softmax scores \+ top-1 artifact) in `G_attr` for each node.

**What**:

* Update each node with `artifact_gnn_probs` (softmax scores) and `artifact_gnn_top1` (most likely artifact class).

**How**:

* Iterate over `gnn_batches` after evaluation.

### **9️ Final Artifact Ranking**

**Why**:  
 Refine GNN predictions with **heuristic biases** from:

* Roles (backend → VM, orchestrator → orchestrated container, etc.).  
* Entropy (high → container, serverless).  
  Graph metrics (high degree → container, high betweenness → serverless).

**What**:

* For each workload pair connected via a device:

  * Combine GNN predictions \+ biases.  
  * Normalize scores.  
  * Store **final artifact probabilities** for each workload.

**How**:

* `final_scores()` combines GNN and bias vectors.  
* Results stored in `chain_recommendations`.

### **Next Steps: Validation?**

**system is trained on heuristic \+ weak supervision** (no strict ground truth).  
**Validation Idea**: Ask domain experts to **manually label** a sample of workloads to compare against system predictions.

**FIND ARTICLES**:

- Graph Construction from PCAP Data  
- Community Detection in Graphs  
- Graph Metrics Computation  
- Role Mining and Node Classification  
- Graph Neural Networks for Artifact Inference  
- Final Artifact Ranking and Validation

